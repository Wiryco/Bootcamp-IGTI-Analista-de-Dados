{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font  color=blue>Análise de Sentimento no Twitter</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esse projeto consiste na Análise de sentimentos de tweets coletados através da API do Twitter sobre X. O projeto consiste em:\n",
    "* Coleta de tweets.\n",
    "* Identificação de tweets mais curtidos e retweetados.\n",
    "* Identificação da fonte dos tweets. \n",
    "* Análise de sentimentos. \n",
    "* Nuvem de palavras mais frequentes. \n",
    "* Séries de tweets no tempo. \n",
    "* Mapa de calor dos tweets utilizando a localização declarada pelos usuários.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font color=red>Instalação dos pacotes</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de realizar o desafio será necessário instalar os seguintes pacotes:\n",
    "* **Geopy:** Usada para definir a geolocalização<br>\n",
    "*conda install -c conda-forge geopy*\n",
    "* **folium**\n",
    "*conda install -c conda-forge folium*\n",
    "* **wordcloud:** Usada para gerar uma nuvem de palavras<br>\n",
    "*conda install -c conda-forge wordcloud*\n",
    "* **pandas**<br>\n",
    "*conda install -c anaconda pandas*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=blue>Importação de pacotes</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pacotes básico\n",
    "import tweepy #Acessa API do Twitter\n",
    "import pandas as pd     \n",
    "import numpy as np \n",
    "from textblob import TextBlob as tb\n",
    "import time\n",
    "import re\n",
    "\n",
    "# Pacotes para visualização \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#MAPA\n",
    "from geopy.geocoders import Nominatim\n",
    "import folium\n",
    "from folium import plugins\n",
    "from wordcloud import WordCloud, STOPWORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=blue>Definição das credenciais de acesso (autenticação) na API do Twitter</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credenciais para utilização da API do Twitter\n",
    "\n",
    "consumer_key = \"\"\n",
    "consumer_secret = \"\"\n",
    "access_token = \"\"\n",
    "access_token_secret = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=blue>Autenticação na API do Twitter</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Realizar autenticação no Twitter\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth,wait_on_rate_limit=True,wait_on_rate_limit_notify=True,retry_count=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**onde:**\n",
    "* retry_count - número padrão de tentativas para tentar quando ocorrer um erro\n",
    "* retry_delay - número de segundos para aguardar entre tentativas\n",
    "* wait_on_rate_limit - se deve ou não esperar automaticamente a reposição dos limites de taxa\n",
    "* wait_on_rate_limit_notify - Imprima ou não uma notificação quando o Tweepy estiver aguardando a reposição dos limites de taxa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=blue>Coleta de dados (busca por palavra chave)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir listas de armazenamento\n",
    "tweets = []\n",
    "info = []\n",
    "\n",
    "#Definir que palavra deseja pesquisar no Twitter\n",
    "#keyword = ('home office  OR  trabalho remoto  OR  trabalho em casa OR homeoffice OR  trabalhoremoto OR trabalhoemcasa')\n",
    "keyword = (\"covid-19  OR  covid  OR  coronavirus OR pandemic\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OBSERVAÇÃO:**\n",
    "A documentação completa do pacote *tweepy* está disponível no link abaixo:\n",
    "* http://docs.tweepy.org/en/v3.5.0/api.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para realizar a busca por palavra chave vamos utilizar a função abaixo:<br>\n",
    "* <font color=green>API.search(q[, lang][, locale][, rpp][, page][, since_id][, geocode][, show_user])</font><br><br>\n",
    "**onde os principais parâmetros que serão usados são:**\n",
    "\n",
    "* <font color=blue>q</font> - a string de consulta de pesquisa\n",
    "* <font color=blue>lang</font> - Restringe os tweets para o idioma especificado, fornecido por um código ISO 639-1.\n",
    "* <font color=blue>rpp</font> - O número de tweets a serem retornados por página, até no máximo 100.\n",
    "* <font color=blue>page</font> - O número da página (começando em 1) a ser retornado, até um máximo de aproximadamente 1500 resultados (com base na página rpp).\n",
    "* <font color=blue>since_id</font>  - Retorna apenas status com um ID maior que (ou seja, mais recente que) o ID especificado.\n",
    "* <font color=blue>geocode</font>  - Retorna tweets de usuários localizados em um determinado raio da latitude / longitude especificada.\n",
    "* <font color=blue>show_user</font>  - Quando verdadeiro, precede \"<user>:\" no início do tweet. O padrão é falso.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  <font color=red>Exemplo básico para busca por palavra chave e recuperar os tweets em português.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observação:** Antes de executar o código abaixo, faça a instalação dos pacotes conforme os comandos abaixo:\n",
    "* googletrans (https://anaconda.org/conda-forge/googletrans)<br>\n",
    "*conda install -c conda-forge googletrans*\n",
    "* unidecode (https://anaconda.org/anaconda/unidecode)<br>\n",
    "*conda install -c anaconda unidecode*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator\n",
    "from unidecode import unidecode\n",
    "\n",
    "polaritiesPT = []\n",
    "\n",
    "for tweetpt in tweepy.Cursor(api.search,q=keyword, tweet_mode='extended',\n",
    "                           rpp=5, result_type=\"popular\",lang=\"pt\",\n",
    "                           include_entities=True).items(5):\n",
    "    #Texto do Tweet\n",
    "    textPT = unidecode(tweetpt.full_text)\n",
    "    print('** Tweet em português: '+textPT)\n",
    "    #Traduzindo para o Inglês\n",
    "    textEN = Translator().translate(textPT)\n",
    "    print('** Tweet traduzido:')\n",
    "    print(textEN.text)\n",
    "    #Calculando a polaridade do texto traduzido\n",
    "    polarityPT = tb(textEN.text).sentiment.polarity #analisa a polaridade\n",
    "\n",
    "    polaritiesPT.append(polarityPT)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(polaritiesPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <font color=blue>Buscar por palavra chave e recuperar os tweets em inglês</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tweet in tweepy.Cursor(api.search,\n",
    "                           q=keyword, tweet_mode='extended',\n",
    "                           rpp=1000, result_type=\"popular\",lang='en', #serão coletados apenas 1000 tweets nesta aula\n",
    "                           include_entities=True).items(1000):\n",
    "                           #include_entities=True).items(1000):   \n",
    "        \n",
    "    if 'retweeted_status' in dir(tweet):\n",
    "        aux=tweet.retweeted_status.full_text\n",
    "    else:\n",
    "        aux=tweet.full_text\n",
    "        \n",
    "    newtweet = aux.replace(\"\\n\", \" \")\n",
    "   \n",
    "    tweets.append(newtweet)\n",
    "    info.append(tweet)\n",
    "    \n",
    "    #file = open(\"tweets_Keyword_covid_10.txt\", \"a\", -1, \"utf-8\")\n",
    "    file = open(\"tweets_Keyword_covid10000.txt\", \"a\", -1, \"utf-8\")    \n",
    "    file.write(newtweet+'\\n')\n",
    "    file.close()\n",
    "    \n",
    "    #time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para verificar a quantidade de tweets coletado use a função \"len()\"\n",
    "\n",
    "print(\"Total de tweets coletados %s.\" % (len(info)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=blue>Criar dataframe</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = pd.DataFrame(tweets, columns=['Tweets']) \n",
    "\n",
    "tweets_df['len']  = np.array([len(tweet) for tweet in tweets])\n",
    "tweets_df['ID']   = np.array([tweet.id for tweet in info])\n",
    "tweets_df['USER']   = np.array([tweet.user.screen_name for tweet in info])\n",
    "tweets_df['userName'] = np.array([tweet.user.name for tweet in info])\n",
    "tweets_df['User Location']    = np.array([tweet.user.location for tweet in info])\n",
    "tweets_df['Language'] = np.array([tweet.user.lang for tweet in info])\n",
    "tweets_df['Date'] = np.array([tweet.created_at for tweet in info])\n",
    "tweets_df['Source'] = np.array([tweet.source for tweet in info])\n",
    "tweets_df['Likes']  = np.array([tweet.favorite_count for tweet in info])\n",
    "tweets_df['Retweets']    = np.array([tweet.retweet_count for tweet in info])\n",
    "tweets_df['Geo']    = np.array([tweet.geo for tweet in info])\n",
    "tweets_df['Coordinates']    = np.array([tweet.coordinates for tweet in info])         \n",
    "tweets_df['Place']    = np.array([tweet.place for tweet in info])\n",
    "\n",
    "tweets_df.to_csv(\"tweets_Keyword_covid_100.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identificar Top Tweets coletados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tweets com maior número de LIKES\n",
    "likes_max = np.max(tweets_df['Likes']) #Função max do numpy identifica o valor máximo\n",
    "\n",
    "likes = tweets_df[tweets_df.Likes == likes_max].index[0] #pega o primeiro tweet com valor máximo de curtidas\n",
    "\n",
    "print(\"O tweet com mais curtidas (likes) é: \\n{}\".format(tweets_df['Tweets'][likes]))\n",
    "print(\"Número de curtidas: {}\".format(likes_max))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(tweets_df['Likes'] == likes_max)) #conta quantos tweets possuem o mesmo valor máximo de curtidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retweet_max  = np.max(tweets_df['Retweets']) #retorna o valor máximo\n",
    "\n",
    "retweet  = tweets_df[tweets_df.Retweets == retweet_max].index[0] #pega o primeiro tweet com valor máximo de Retweets\n",
    "\n",
    "print(\"O tweet com mais retweet é: \\n{}\".format(tweets_df['Tweets'][retweet]))\n",
    "print(\"Número de retweets: {}\".format(retweet_max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(tweets_df['Retweets'] == retweet_max)) #conta quantos tweets possuem o mesmo valor máximo de Retweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifica a fonte(origem) do tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A fonte neste caso é o app ou dispositivo onde o usuário postou o tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = [] #lista para armazenar a fontes\n",
    "for source in tweets_df['Source']:\n",
    "    if source not in sources:\n",
    "        sources.append(source) #inclui no vetor sources apenas se a fonte encontrada ainda não foi incluída\n",
    "\n",
    "percent = np.zeros(len(sources)) #Retorna um novo vetor, com o número de elementos do vetor sources, preenchido com zeros a new array filled with zeros, for\n",
    "\n",
    "for source in tweets_df['Source']:\n",
    "    for index in range(len(sources)):\n",
    "        if source == sources[index]:\n",
    "            percent[index] += 1\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gráfico que exibe o número de tweets por fonte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sourceDF = pd.DataFrame({\n",
    " 'source':percent,\n",
    "}, index=sources)\n",
    "\n",
    "sourceDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources_sorted = sourceDF.sort_values('source',ascending=True)\n",
    "ax = sources_sorted.source.plot(kind='barh',color='#A52A2A')\n",
    "ax.get_xaxis().set_major_formatter(plt.FuncFormatter(lambda x, loc: \"{:,}\".format(int(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources_sorted = sourceDF.sort_values('source',ascending=False)\n",
    "ax = sources_sorted.source.plot(kind='barh',color='#B8860B')\n",
    "ax.get_xaxis().set_major_formatter(plt.FuncFormatter(lambda x, loc: \"{:,}\".format(int(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dica:** \n",
    "Veja relação de cores em HTML em:\n",
    "* https://www.w3schools.com/tags/ref_colornames.asp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=red>Análise de polaridade:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variável que irá armazenar as polaridades\n",
    "analysis = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista vazia para armazenar as polaridades\n",
    "polarities = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calcula polaridade (sentiment.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tweet in tweets_df['Tweets']: #para cada tweet    \n",
    "    analysis = tb(tweet)   \n",
    "    \n",
    "    polarity = analysis.sentiment.polarity #analisa a polaridade\n",
    "\n",
    "    polarities.append(polarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ou**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tweet in tweets_df['Tweets']: #para cada tweet    \n",
    "    #analysis = tb(tweet)   \n",
    "   \n",
    "    polarity = tb(tweet).sentiment.polarity #analisa a polaridade\n",
    "\n",
    "    polarities.append(polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Vetor de polaridade:',polarities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=red>Análise de Sentimentos:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Para a(s) palavra(s):\"%s\"' % keyword)\n",
    "print('A MÉDIA DE SENTIMENTO É: ' + str(np.mean(polarities)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive = 0\n",
    "negative = 0\n",
    "neutral = 0\n",
    "\n",
    "for polarity in polarities:\n",
    "    if polarity > 0:\n",
    "        positive = positive+1\n",
    "    elif polarity < 0:\n",
    "        negative = negative+1\n",
    "    else:\n",
    "        neutral = neutral+1                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tweets Positivos: %s\" % positive)\n",
    "print(\"Tweets Negativos: %s\" % negative)\n",
    "print(\"Tweets Neutros: %s\" % neutral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calcula percentual\n",
    "pos_pct=positive*100/len(polarities)\n",
    "neg_pct=negative*100/len(polarities)\n",
    "neu_pct=neutral*100/len(polarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments = ['Positivos', 'Negativos','Neutros']\n",
    "percents = [pos_pct, neg_pct, neu_pct]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pie_chart = pd.Series(percents, index=sentiments,name='Sentimentos')\n",
    "pie_chart.plot.pie(fontsize=12, autopct='%.2f', figsize=(5, 5),title=\"Análise de Sentimentos tweets\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',        shadow=True, startangle=90)\n",
    "explode = (0.1, 0, 0) #Separa o primeiro\n",
    "\n",
    "pie_chart = pd.Series(percents, index=sentiments,name='')\n",
    "pie_chart.plot.pie(fontsize=12, explode=explode, autopct='%.2f%%', shadow=True, figsize=(7, 7),title=\"Análise de Sentimentos tweets\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dica:**\n",
    "Veja a documentação do pacote *Matplotlib* no link:\n",
    "* https://matplotlib.org/3.1.1/contents.html#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=blue>Mapa de calor dos tweets</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geolocator = Nominatim(user_agent=\"TweeterSentiments\")\n",
    "\n",
    "latitude = []\n",
    "longitude = []\n",
    "\n",
    "for user_location in tweets_df['User Location']:\n",
    "    try:\n",
    "        location = geolocator.geocode(user_location)\n",
    "        latitude.append(location.latitude)\n",
    "        longitude.append(location.longitude)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dica:** Veja a documentação da função *Nominatim* no link:\n",
    "\n",
    "* https://geopy.readthedocs.io/en/stable/#nominatim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coordenadas = np.column_stack((latitude, longitude))\n",
    "\n",
    "mapa = folium.Map(zoom_start=3.)\n",
    "mapa.add_child(plugins.HeatMap(coordenadas))\n",
    "mapa.save('covid.html')\n",
    "mapa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dica:** Para conhecer o pacote Folium veja os links:\n",
    "* https://python-visualization.github.io/folium/\n",
    "* https://medium.com/@datalivre/folium-d6036a9ad29c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=blue>Nuvem de palavras</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ' '.join(tweets_df['Tweets'])\n",
    "\n",
    "words_clean = \" \".join([word for word in words.split()\n",
    "                            if 'https' not in word\n",
    "                                and not word.startswith('@')\n",
    "                                and word != 'RT'\n",
    "                            ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OBSERVAÇÃO:**\n",
    "Antes de importar o pacote imread faça a intalação do mesmo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "wc = WordCloud(min_font_size=10, \n",
    "               max_font_size=300, \n",
    "               background_color='white', \n",
    "               mode=\"RGB\",\n",
    "               width=2000, \n",
    "               height=1000,\n",
    "               normalize_plurals= True).generate(words_clean)\n",
    "\n",
    "plt.imshow(wc, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.savefig('covid_clound.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dica:** Para conhecer o pacote WordCloud veja os links:\n",
    "* https://amueller.github.io/word_cloud/index.html\n",
    "* Exemplos: https://amueller.github.io/word_cloud/auto_examples/index.html#example-gallery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = WordCloud(min_font_size=10, \n",
    "               max_font_size=300, \n",
    "               background_color='black', #cor de fundo\n",
    "               mode=\"RGB\",\n",
    "               width=2000, \n",
    "               height=1000,\n",
    "               normalize_plurals= True).generate(words_clean)\n",
    "\n",
    "plt.imshow(wc, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "#plt.savefig('covid_clound.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Para texto em português, use o exemplo abaixo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords= set(STOPWORDS)\n",
    "\n",
    "new_words = []\n",
    "with open(\"stopwords_pt.txt\", 'r') as f:\n",
    "    [new_words.append(word) for line in f for word in line.split()]\n",
    "\n",
    "new_stopwords = stopwords.union(new_words)\n",
    "\n",
    "words = ' '.join(tweets_df['Tweets'])\n",
    "\n",
    "words_clean = \" \".join([word for word in words.split()\n",
    "                            if 'https' not in word\n",
    "                                and not word.startswith('@')\n",
    "                                and word != 'RT'\n",
    "                            ])\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "wc = WordCloud(min_font_size=10, \n",
    "               max_font_size=300, \n",
    "               background_color='white', \n",
    "               mode=\"RGB\",\n",
    "               stopwords=new_stopwords,\n",
    "               width=2000, \n",
    "               height=1000,\n",
    "               normalize_plurals= True).generate(words_clean)\n",
    "\n",
    "plt.imshow(wc, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.savefig('covid_clound_pt.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=blue>Análise temporal dos tweets</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tweets postados por dia\n",
    "data = tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['Date'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tweets postados por dia\n",
    "data = tweets_df\n",
    "\n",
    "print(data['Date'][0])\n",
    "\n",
    "data['Date'] = pd.to_datetime(data['Date']).apply(lambda x: x.date())\n",
    "\n",
    "print(data['Date'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data['Date'].value_counts() #conta a quantidade de tweets por dia\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tlen = pd.Series(data['Date'].value_counts(), index=data['Date'])\n",
    "\n",
    "tlen.plot(figsize=(16,4), color='r');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=red>Text Mining com o pacote NLTK</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para conhecer o pacote *NLTK* veja a documentação completa em:\n",
    "* https://www.nltk.org/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Desgustação**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tweet_tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_tokens = [] #lista para armazenar os tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tweet in tweets_df['Tweets']:   \n",
    "    print (tweet_tokenizer.tokenize(tweet))\n",
    "    tweets_tokens.append(tweet_tokenizer.tokenize(tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limpar tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "import string\n",
    " \n",
    "#punctuation = list(string.punctuation)\n",
    "stopwords_english = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweets(tweet):\n",
    "    # remove stock market tickers like $GE\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    " \n",
    "    # remove old style retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    " \n",
    "    # remove hyperlinks\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    \n",
    "    # remove hashtags\n",
    "    # only removing the hash # sign from the word\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    \n",
    "    # tokenize tweets\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    " \n",
    "    tweets_clean = []    \n",
    "    for word in tweet_tokens:\n",
    "        if (word not in stopwords_english and # remove stopwords\n",
    "                word not in string.punctuation): # remove punctuation\n",
    "            tweets_clean.append(word)\n",
    " \n",
    "    return tweets_clean    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_tweets_tokens = [] #lista para armazenar os tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tweet in tweets_df['Tweets']:   \n",
    "    print (clean_tweets(tweet))\n",
    "    clean_tweets_tokens.append(clean_tweets(tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fim!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
